{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trieuduongle/FLAVR/blob/generative-approach/Training_FLAVR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "holsmMm9K7ZW",
        "outputId": "53283433-5d9e-4066-8a8c-82f37cb2329f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/trieuduongle/FLAVR.git '/content/drive/My Drive/Duong/FLAVR/code'"
      ],
      "metadata": {
        "id": "zpiLPPDiM6xQ",
        "outputId": "7750c55e-61aa-4500-f56f-f85c442ccd48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/drive/My Drive/Duong/FLAVR/code'...\n",
            "remote: Enumerating objects: 296, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 296 (delta 28), reused 22 (delta 22), pack-reused 259\u001b[K\n",
            "Receiving objects: 100% (296/296), 33.80 MiB | 17.45 MiB/s, done.\n",
            "Resolving deltas: 100% (161/161), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "PSc08l6CdfC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/My Drive/Duong/FLAVR/code'"
      ],
      "metadata": {
        "id": "JzGeXaRVNIQj",
        "outputId": "1692c098-b156-4331-d45c-42086a27e072",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Duong/FLAVR/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git fetch origin && git checkout generative-approach && git pull"
      ],
      "metadata": {
        "id": "V8NdXX4-NSEa",
        "outputId": "df69322f-6f3c-4a86-db25-4c547a6d6d58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already on 'generative-approach'\n",
            "Your branch is up to date with 'origin/generative-approach'.\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --batch_size 12 --test_batch_size 12 --dataset vimeo90K_septuplet --loss 1*L1 --max_epoch 200 --lr 0.0002 --data_root \"/content/drive/My Drive/Duong/datasets/vimeo-septuplet\" --n_outputs 1"
      ],
      "metadata": {
        "id": "A6bwjpCOQAN9",
        "outputId": "97cdb563-b9fe-427c-c21b-9e2f4846b111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=12, beta1=0.9, beta2=0.99, checkpoint_dir='.', cuda=True, data_root='/content/drive/My Drive/Duong/datasets/vimeo-septuplet', dataset='vimeo90K_septuplet', exp_name='exp', joinType='concat', load_from=None, log_iter=60, loss='1*L1', lr=0.0002, max_epoch=200, model='unet_18', n_outputs=1, nbr_frame=4, nbr_width=1, num_gpu=1, num_workers=16, pretrained=None, random_seed=12345, resume=False, resume_exp=None, start_epoch=0, test_batch_size=12, upmode='transpose', use_tensorboard=False, val_freq=1)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Building model: unet_18\n",
            "Preparing loss function:\n",
            "1.000 * L1\n",
            "Train Epoch: 0 [0/652]\tLoss: 0.179622\tPSNR: 13.8431\n",
            "Train Epoch: 0 [60/652]\tLoss: 0.162781\tPSNR: 13.7569\n",
            "Train Epoch: 0 [120/652]\tLoss: 0.158649\tPSNR: 13.7712\n",
            "Train Epoch: 0 [180/652]\tLoss: 0.163415\tPSNR: 15.6776\n",
            "Train Epoch: 0 [240/652]\tLoss: 0.162338\tPSNR: 16.0644\n",
            "Train Epoch: 0 [300/652]\tLoss: 0.160312\tPSNR: 13.9548\n",
            "Train Epoch: 0 [360/652]\tLoss: 0.160800\tPSNR: 13.9924\n",
            "Train Epoch: 0 [420/652]\tLoss: 0.159939\tPSNR: 13.9037\n",
            "Train Epoch: 0 [480/652]\tLoss: 0.154992\tPSNR: 13.3776\n",
            "Train Epoch: 0 [540/652]\tLoss: 0.157987\tPSNR: 15.3511\n",
            "Train Epoch: 0 [600/652]\tLoss: 0.163689\tPSNR: 12.8176\n",
            "Evaluating for epoch = 0\n",
            "100% 652/652 [42:48<00:00,  3.94s/it]\n",
            "Loss: 0.065172, PSNR: 20.878301, SSIM: 0.683192\n",
            "\n",
            "Train Epoch: 1 [0/652]\tLoss: 0.203493\tPSNR: 12.8654\n",
            "Train Epoch: 1 [60/652]\tLoss: 0.162411\tPSNR: 15.4833\n",
            "Train Epoch: 1 [120/652]\tLoss: 0.159592\tPSNR: 13.4789\n",
            "Train Epoch: 1 [180/652]\tLoss: 0.157330\tPSNR: 17.7058\n",
            "Train Epoch: 1 [240/652]\tLoss: 0.160416\tPSNR: 14.2433\n",
            "Train Epoch: 1 [300/652]\tLoss: 0.157954\tPSNR: 14.1674\n",
            "Train Epoch: 1 [360/652]\tLoss: 0.158478\tPSNR: 14.1987\n",
            "Train Epoch: 1 [420/652]\tLoss: 0.159601\tPSNR: 14.0368\n",
            "Train Epoch: 1 [480/652]\tLoss: 0.159808\tPSNR: 13.9506\n",
            "Train Epoch: 1 [540/652]\tLoss: 0.156348\tPSNR: 14.1501\n",
            "Train Epoch: 1 [600/652]\tLoss: 0.153982\tPSNR: 13.5809\n",
            "Evaluating for epoch = 1\n",
            "100% 652/652 [42:28<00:00,  3.91s/it]\n",
            "Loss: 0.093072, PSNR: 17.759996, SSIM: 0.669507\n",
            "\n",
            "Train Epoch: 2 [0/652]\tLoss: 0.175017\tPSNR: 13.0664\n",
            "Train Epoch: 2 [60/652]\tLoss: 0.159691\tPSNR: 16.8441\n",
            "Train Epoch: 2 [120/652]\tLoss: 0.162333\tPSNR: 13.3147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python Middleburry_Test.py \\\n",
        "  --data_root \"/content/drive/My Drive/Duong/datasets/middleburry/test\" \\\n",
        "  --load_from \"/content/drive/MyDrive/Duong/FLAVR/code/saved_models_final/vimeo90K_septuplet/exp/model_best.pth\" \\\n",
        "  --n_outputs 1"
      ],
      "metadata": {
        "id": "PpwMmxXbc06p",
        "outputId": "77084f84-98ac-498b-872a-b5510031b7c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Building model: unet_18\n",
            "#params 42061571\n",
            "Time Taken 5.65799880027771\n",
            "Time Taken 0.008217096328735352\n",
            "Time Taken 0.0075435638427734375\n",
            "Time Taken 0.0077114105224609375\n",
            "Time Taken 0.007925271987915039\n",
            "Time Taken 0.010061979293823242\n",
            "Time Taken 0.009283065795898438\n",
            "Time Taken 0.007558584213256836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python Middleburry_Test.py \\\n",
        "  --data_root \"/content/drive/My Drive/Duong/datasets/middleburry/test\" \\\n",
        "  --load_from \"/content/drive/MyDrive/Duong/FLAVR/code/FLAVR_2x.pth\" \\\n",
        "  --n_outputs 1"
      ],
      "metadata": {
        "id": "fVhIQR89scqb",
        "outputId": "64c7cd32-af74-4155-a835-ec7d5a74cd5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Building model: unet_18\n",
            "#params 42061571\n",
            "Traceback (most recent call last):\n",
            "  File \"Middleburry_Test.py\", line 104, in <module>\n",
            "    main(args)\n",
            "  File \"Middleburry_Test.py\", line 100, in main\n",
            "    test(args)\n",
            "  File \"Middleburry_Test.py\", line 79, in test\n",
            "    out = model(images)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/parallel/data_parallel.py\", line 166, in forward\n",
            "    return self.module(*inputs[0], **kwargs[0])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Duong/FLAVR/code/model/FLAVR_arch.py\", line 153, in forward\n",
            "    x_0 , x_1 , x_2 , x_3 , x_4 = self.encoder(images)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Duong/FLAVR/code/model/resnet_3D.py\", line 184, in forward\n",
            "    x_0 = self.stem(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 139, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 607, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 603, in _conv_forward\n",
            "    input, weight, bias, self.stride, self.padding, self.dilation, self.groups\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull && python main.py \\\n",
        "  --batch_size 10 \\\n",
        "  --test_batch_size 10 \\\n",
        "  --dataset vimeo90K_septuplet \\\n",
        "  --loss 1*GAN \\\n",
        "  --checkpoint_dir integrate_gan \\\n",
        "  --patch_size 256 \\\n",
        "  --max_epoch 200 \\\n",
        "  --lr 0.0002 \\\n",
        "  --data_root \"/content/drive/My Drive/Duong/datasets/vimeo-septuplet\" \\\n",
        "  --n_outputs 1"
      ],
      "metadata": {
        "id": "nkm91s87z6KA",
        "outputId": "44f2a97a-05e6-4009-ba2e-d1c0e47fd8ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "Namespace(batch_size=10, beta1=0.9, beta2=0.99, checkpoint_dir='integrate_gan', cuda=True, data_root='/content/drive/My Drive/Duong/datasets/vimeo-septuplet', dataset='vimeo90K_septuplet', exp_name='exp', joinType='concat', load_from=None, log_iter=60, loss='1*GAN', lr=0.0002, max_epoch=200, model='unet_18', n_outputs=1, nbr_frame=4, nbr_width=1, num_gpu=1, num_workers=16, patch_size=256, pretrained=None, random_seed=12345, resume=False, resume_exp=None, start_epoch=0, test_batch_size=10, upmode='transpose', use_tensorboard=False, val_freq=1)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Building model: unet_18\n",
            "Preparing loss function:\n",
            "1.000 * GAN\n",
            "images\n",
            "4 torch.Size([10, 3, 256, 256])\n",
            "1 torch.Size([10, 3, 256, 256])\n",
            "4 torch.Size([10, 3, 256, 256])\n",
            "10 torch.Size([3, 256, 256])\n",
            "calling\n",
            "[10, 3, 256, 256]\n",
            "[10, 3, 256, 256]\n",
            "extra_vars\n",
            "hello\n",
            "Train Epoch: 0 [0/783]\tLoss: 13.176337\tPSNR: 15.1716\n",
            "images\n",
            "4 torch.Size([10, 3, 256, 256])\n",
            "1 torch.Size([10, 3, 256, 256])\n",
            "4 torch.Size([10, 3, 256, 256])\n",
            "10 torch.Size([3, 256, 256])\n",
            "calling\n",
            "[10, 3, 256, 256]\n",
            "[10, 3, 256, 256]\n",
            "extra_vars\n",
            "hello\n",
            "images\n",
            "4 torch.Size([10, 3, 256, 256])\n",
            "1 torch.Size([10, 3, 256, 256])\n",
            "4 torch.Size([10, 3, 256, 256])\n",
            "10 torch.Size([3, 256, 256])\n",
            "calling\n",
            "[10, 3, 256, 256]\n",
            "[10, 3, 256, 256]\n",
            "extra_vars\n",
            "hello\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 216, in <module>\n",
            "    main(args)\n",
            "  File \"main.py\", line 197, in main\n",
            "    train(args, epoch)\n",
            "  File \"main.py\", line 85, in train\n",
            "    images = [img_.cuda() for img_ in images]\n",
            "  File \"main.py\", line 85, in <listcomp>\n",
            "    images = [img_.cuda() for img_ in images]\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py \\\n",
        "  --batch_size 6 \\\n",
        "  --test_batch_size 6 \\\n",
        "  --dataset gopro \\\n",
        "  --loss 1*L1 \\\n",
        "  --max_epoch 200 \\\n",
        "  --lr 0.0002 \\\n",
        "  --data_root \"/content/drive/My Drive/Duong/datasets/gopro/samples\" \\\n",
        "  --n_outputs 2"
      ],
      "metadata": {
        "id": "euX_IG2mYxmB",
        "outputId": "921bdca9-9df1-457a-b512-f113d9e61c66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=6, beta1=0.9, beta2=0.99, checkpoint_dir='.', cuda=True, data_root='/content/drive/My Drive/Duong/datasets/gopro/samples', dataset='gopro', exp_name='exp', joinType='concat', load_from=None, log_iter=60, loss='1*L1', lr=0.0002, max_epoch=200, model='unet_18', n_outputs=2, nbr_frame=4, nbr_width=1, num_gpu=1, num_workers=16, patch_size=256, pretrained=None, random_seed=12345, resume=False, resume_exp=None, start_epoch=0, test_batch_size=6, upmode='transpose', use_tensorboard=False, val_freq=1)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Building model: unet_18\n",
            "Preparing loss function:\n",
            "1.000 * L1\n",
            "images\n",
            "4 torch.Size([6, 3, 512, 512])\n",
            "2 torch.Size([6, 3, 512, 512])\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 216, in <module>\n",
            "    main(args)\n",
            "  File \"main.py\", line 197, in main\n",
            "    train(args, epoch)\n",
            "  File \"main.py\", line 94, in train\n",
            "    out = model(images)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/parallel/data_parallel.py\", line 166, in forward\n",
            "    return self.module(*inputs[0], **kwargs[0])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Duong/FLAVR/code/model/FLAVR_arch.py\", line 168, in forward\n",
            "    dx_out = torch.cat(torch.unbind(dx_out , 2) , 1)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 14.76 GiB total capacity; 12.90 GiB already allocated; 387.75 MiB free; 13.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --batch_size 12 --test_batch_size 12 --dataset vimeo90K_septuplet --loss 1*L1 --max_epoch 200 --lr 0.0002 --data_root \"/content/drive/My Drive/Duong/datasets/vimeo-septuplet\" --n_outputs 1"
      ],
      "metadata": {
        "id": "WpjFsNELmSsq",
        "outputId": "f1a45c1c-f661-462c-c6ea-babe84ffb216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=12, beta1=0.9, beta2=0.99, checkpoint_dir='.', cuda=True, data_root='/content/drive/My Drive/Duong/datasets/vimeo-septuplet', dataset='vimeo90K_septuplet', exp_name='exp', joinType='concat', load_from=None, log_iter=60, loss='1*L1', lr=0.0002, max_epoch=200, model='unet_18', n_outputs=1, nbr_frame=4, nbr_width=1, num_gpu=1, num_workers=16, patch_size=256, pretrained=None, random_seed=12345, resume=False, resume_exp=None, start_epoch=0, test_batch_size=12, upmode='transpose', use_tensorboard=False, val_freq=1)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Building model: unet_18\n",
            "Preparing loss function:\n",
            "1.000 * L1\n",
            "images\n",
            "4 torch.Size([12, 3, 256, 256])\n",
            "1 torch.Size([12, 3, 256, 256])\n",
            "4 torch.Size([12, 3, 256, 256])\n",
            "12 torch.Size([3, 256, 256])\n",
            "Train Epoch: 0 [0/652]\tLoss: 0.179622\tPSNR: 13.8431\n",
            "images\n",
            "4 torch.Size([12, 3, 256, 256])\n",
            "1 torch.Size([12, 3, 256, 256])\n",
            "4 torch.Size([12, 3, 256, 256])\n",
            "12 torch.Size([3, 256, 256])\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 216, in <module>\n",
            "    main(args)\n",
            "  File \"main.py\", line 197, in main\n",
            "    train(args, epoch)\n",
            "  File \"main.py\", line 85, in train\n",
            "    images = [img_.cuda() for img_ in images]\n",
            "  File \"main.py\", line 85, in <listcomp>\n",
            "    images = [img_.cuda() for img_ in images]\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    }
  ]
}